<!DOCTYPE HTML>
<!--
	Forty by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
    <title>Portfolio Marta Raboso</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets/css/main.css" />
    <noscript>
        <link rel="stylesheet" href="assets/css/noscript.css" />
    </noscript>
</head>

<body class="is-preload">

    <!-- Wrapper -->
    <div id="wrapper">

        <!-- Header -->
        <!-- Note: The "styleN" class below should match that of the banner element. -->
        <header id="header" class="alt style2">
            <a href="index.html" class="logo"><strong>Portfolio</strong> <span>Marta Raboso</span></a>
            <nav>
                <a href="#menu">Menu</a>
            </nav>
        </header>

        <!-- Menu -->
        <nav id="menu">
            <ul class="links">
                <li><a href="index.html">Home</a></li>
                <li><a href="boubaDistrict.html">Bouba District</a></li>
                <li><a href="mergeDown.html">Merge Down</a></li>
                <li><a href="animacion3D.html">Animación 3D</a></li>
                <li><a href="zombfection.html">Zombfection</a></li>
                <li><a href="eldritchApothecary.html">Eldritch Apothecary</a></li>
                <li><a href="inteligenciaArtificial.html">Inteligencia Artificial</a></li>
                <li><a href="diseno3D.html">Diseño 3D</a></li>
                <li><a href="diseno2D.html">Diseño 2D</a></li>
            </ul>
            <ul class="actions stacked">
                <li><a href="#three" class="button next scrolly">Contacto</a></li>
            </ul>
        </nav>

        <!-- Banner -->
        <!-- Note: The "styleN" class below should match that of the header element. -->
        <section id="banner" class="style2">
            <div class="inner">
                <span class="image">
                    <img src="images/pic07.jpg" alt="" />
                </span>
                <header class="major">
                    <h1>Inteligencia Artificial</h1>
                </header>
                <div class="content">
                    <p>Simulación en Unity<br />
                        aplicando aprendizaje por refuerzo</p>
                </div>
            </div>
        </section>

        <!-- Main -->
        <div id="main">

            <!-- One -->
            <section id="one">
                <div class="inner">
                    <header class="major">
                        <h2>INTRODUCCIÓN</h2>
                    </header>
                    <p>Para esta proyecto se implementó el aprendizaje de un agente, que huye de otro personaje
                        utilizando técnicas de Q-Learning en C#. Se proporcionó un escenario en Unity compuesto por un
                        tablero de 20 x 20 casillas. En el escenario se encuentran paredes que el personaje tendrá que
                        esquivar y también evitar que se salga de los límites del mundo.
                    </p>

                    <div style="display:flex; flex-wrap:wrap; justify-content:center; gap:16px;">
                        <img src="images/IA.JPG" style="width:600px; max-width:100%; height:auto;" />
                    </div>
                </div>
            </section>

            <!-- One -->
            <section id="one" style="background-color: rgba(0, 0, 0, 0.2);">
                <div class="inner">
                    <header class="major">
                        <h3 style="font-size: 1.1em;">ALGORITMO Q-LEARNING</h3>
                    </header>
                    <p>El algoritmo Q-Learning es un tipo de aprendizaje por refuerzo mediante el cual nuestro agente
                        deberá aprender de diferentes simulaciones. En ellas recibirá recompensas dependiendo de lo
                        correctas que sean las acciones que realice para poder escapar de su cazador.
                    </p>

                    <p>Para ello, utilizamos una tabla Q que muestra los diferentes estados posibles en los que se puede
                        encontrar nuestro agente, y el valor Q (lo correctas que son) de las acciones posibles.
                    </p>

                    <div style="display:flex; flex-wrap:wrap; justify-content:center; gap:16px;">
                        <img src="images/FormulaQ.JPG" style="width:600px; max-width:100%; height:auto;" />
                    </div>
                    <p> </p>

                    <header class="major">
                        <h3 style="font-size: 1.1em;">VALORES DE ENTRENAMIENTO</h3>
                    </header>
                    <p>A la hora de entrenar al agente, se han asignado los siguientes valores a las variables que
                        calculan los valores Q, y los diferentes números de episodios e iteraciones:
                    </p>

                    <table border="1" cellpadding="8" cellspacing="0">
                        <colgroup>
                            <col style="width: 50%;">
                            <col style="width: 50%;">
                        </colgroup>
                        <tbody>
                            <tr>
                                <td>Alpha</td>
                                <td>0.2</td>
                            </tr>
                            <tr>
                                <td>Epsilon</td>
                                <td>0.85</td>
                            </tr>
                            <tr>
                                <td>Gamma</td>
                                <td>0.9</td>
                            </tr>
                            <tr>
                                <td>Episodes</td>
                                <td>2000</td>
                            </tr>
                            <tr>
                                <td>Max Steps</td>
                                <td>1000</td>
                            </tr>
                            <tr>
                                <td>Episodes Between Saves</td>
                                <td>100</td>
                            </tr>
                        </tbody>
                    </table>

                    <p>Con este número de episodios, aseguramos que nuestro agente itere lo suficiente y tenga muchas
                        oportunidades de aprender nuevas situaciones recreadas. Con el valor de Épsilon controlamos el
                        balance entre la exploración de nuevas acciones y la explotación del conocimiento actual.
                    </p>

                    <p>Alpha es la tasa de aprendizaje. Un mayor valor dará más peso a la nueva información. Gamma es el
                        factor de descuento. Los valores altos harán que el personaje favorezca su explotación a largo
                        plazo y valore más las recompensas futuras.
                    </p>

                    <p>También limitamos el número máximo de pasos que puede dar el agente hasta terminar un episodio
                        con maxSteps y cada cierto número de episodios guardaremos la tabla.
                    </p>

                    <header class="major">
                        <h3 style="font-size: 1.1em;">ESCENA DE ENTRENAMIENTO</h3>
                    </header>
                    <p>En la escena es donde se realizará el entrenamiento. Para ello, haremos que el personaje, tome
                        tanto acciones aleatorias como las ya aprendidas, y vaya aprendiendo cuáles son las mejores para
                        escapar del otro personaje.
                    </p>

                    <h4>ESTADOS</h4>
                    <p>Antes de comenzar el aprendizaje, tendremos que definir qué estados compondrán nuestras
                        simulaciones. Para los estados, elegimos comprobar qué celdas vecinas son suelo y la posición
                        relativa del otro personaje.
                    </p>

                    <table border="1" cellpadding="8" cellspacing="0">
                        <thead>
                            <tr>
                                <th>Norte</th>
                                <th>Este</th>
                                <th>Sur</th>
                                <th>Oeste</th>
                                <th>Other.x</th>
                                <th>Other.y</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>(0, 1)</td>
                                <td>(0, 1)</td>
                                <td>(0, 1)</td>
                                <td>(0, 1)</td>
                                <td>(0, 1, 2)</td>
                                <td>(0, 1, 2)</td>
                            </tr>
                        </tbody>
                    </table>

                    <p>Los valores de las celdas vecinas serán 0 o 1, dependiendo de si son transitables o no. Los
                        valores de la posición relativa del Other serán 0 si es menor que la del agente, 1 si es mayor y
                        2 si es igual que la del personaje. De esta manera, podremos evaluar estados independientemente
                        de la posición en la que se encuentre en el tablero.
                    </p>

                    <p>Con esta propuesta tendríamos 2 * 2 * 2 * 2 * 3 * 3 = 144 estados posibles. Sin embargo, a
                        continuación, veremos que algunos no son posibles o no se dan en nuestro juego.
                    </p>

                    <div style="display:flex; flex-wrap:wrap; justify-content:center; gap:16px;">
                        <img src="images/S1.JPG" style="width:600px; max-width:100%; height:auto;" />
                        <img src="images/S2.JPG" style="width:600px; max-width:100%; height:auto;" />
                        <img src="images/S3.JPG" style="width:600px; max-width:100%; height:auto;" />
                    </div>
                    <p> </p>

                    <h5>ORDENACIÓN DE ESTADOS</h5>
                    <p>El siguiente paso consistió en darle valor a esos estados para ordenarlos e identificarlos en la
                        tabla Q.
                    </p>

                    <p>Para convertir estos estados a un valor numérico, tendremos que ir sumando los números que tenga
                        ese estado en su definición (x tendrá 0, 1 o 2), como si fuera un sistema numérico propio.
                    </p>

                    <div style="display:flex; flex-wrap:wrap; justify-content:center; gap:16px;">
                        <img src="images/S1O.JPG" style="width:600px; max-width:100%; height:auto;" />
                        <img src="images/S2O.JPG" style="width:600px; max-width:100%; height:auto;" />
                        <img src="images/S3O.JPG" style="width:600px; max-width:100%; height:auto;" />
                    </div>
                    <p> </p>

                    <p>Así podremos aprovechar cuando un estado se repite en diferentes partes del tablero. También
                        podremos cambiar las paredes en la escena de Test, ya que los estados no dependen de la posición
                        absoluta de los personajes.
                    </p>

                    <h4>TABLA Q</h4>
                    <p>Para identificar el estado en el que se encuentra el agente, se comprueba si las celdas vecinas
                        son transitables y la posición relativa del agente y hace los cálculos anteriormente explicados.
                    </p>

                    <p>Para el entrenamiento, necesitaremos almacenar los diferentes estados y sus posibles acciones en
                        la tabla Q. En esta, se categorizan los estados, obteniendo un total de 144 estados. Sin
                        embargo, solo se completarán aquellos estados cuya posibilidad no sea nula. Así obtendremos el
                        siguiente documento .csv:
                    </p>

                    <div style="display:flex; flex-wrap:wrap; justify-content:center; gap:16px;">
                        <img src="images/Qtable.JPG" style="width:600px; max-width:100%; height:auto;" />
                    </div>
                    <p> </p>

                    <h4>IMPLEMENTACIÓN</h4>
                    <p>El agente se mueve de manera aleatoria y se calcula el valor actual del estado. Si este se aleja
                        de su enemigo, recibirá una recompensa, sin embargo, si el agente es capturado recibirá una
                        recompensa negativa.
                    </p>

                    <p>Por último, se calcula un nuevo valor Q para la acción y estado deseado mediante la fórmula de la
                        regla de aprendizaje, utilizando los valores obtenidos con los anteriores métodos.
                    </p>

                    <p>Tras finalizar la simulación, nuestro agente ya habrá completado su aprendizaje y así habrá
                        recopilado su aprendizaje en la tabla Q generada. Ahora utilizará lo aprendido sin ninguna
                        instrucción más que leer esta tabla.
                    </p>

                    <h4>ESCENA DE TEST</h4>
                    <p>En la escena de test se aplicará lo aprendido por el agente en una simulación. Para evitar ser
                        capturado, el agente, deberá leer correctamente la tabla Q y deducir qué acción evitará que sea
                        capturado.
                    </p>

                    <p>Se calculará el estado actual dadas las posiciones del agente y del otro personaje.
                        Posteriormente, obtiene qué acción deberá realizar, leyendo la tabla Q, y se moverá a la casilla
                        obtenida.
                    </p>

                    <div style="display:flex; flex-wrap:wrap; justify-content:center; gap:16px;">
                        <img src="images/IA2.JPG" style="width:600px; max-width:100%; height:auto;" />
                    </div>
                </div>
            </section>
        </div>

        <!-- Contact -->
        <section id="three">
            <div class="inner">
                <h2>CONTACTO</h2>
                <table style="margin: 0 auto; border-collapse: collapse;">
                    <tr>
                        <td style="padding: 3rem 3rem;">
                            <div class="contact-method">
                                <span class="icon solid alt fa-envelope"></span>
                                <h3>Email</h3>
                                <a href="mailto:rabosomar@gmail.com">rabosomar@gmail.com</a>
                            </div>
                        </td>

                        <td style="padding: 3rem 3rem;">
                            <div class="contact-method">
                                <span class="icon brands alt fa-linkedin-in"></span>
                                <h3>LinkedIn</h3>
                                <a href="https://www.linkedin.com/in/marta-raboso-g%C3%B3mez-2a7a0323a/" target="_blank"
                                    rel="noopener noreferrer">
                                    Ver mi perfil
                                </a>
                            </div>
                        </td>
                    </tr>
                </table>
            </div>
        </section>

        <!-- Footer -->
        <footer id="footer">
            <div class="inner">
                <ul class="copyright">
                    <!-- <li>&copy; Untitled</li> -->
                    <li>Design: <a href="https://html5up.net">HTML5 UP</a></li>
                </ul>
            </div>
        </footer>

    </div>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/jquery.scrollex.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>

</body>

</html>